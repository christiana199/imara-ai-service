{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cc2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ImaraFund Intelligent Matching Engine - COMPLETE VERSION\n",
    "AI-powered funding matcher with clear, simple recommendations using Gemini 2.5 Flash\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    GEMINI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GEMINI_AVAILABLE = False\n",
    "    print(\"âš ï¸ google-generativeai not installed. Run: pip install google-generativeai\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class IntelligentMatcher:\n",
    "    def __init__(self):\n",
    "        print(\"ðŸ¤– Initializing ImaraFund AI Matcher...\")\n",
    "        \n",
    "        # Load datasets\n",
    "        try:\n",
    "            self.grants = pd.read_csv('data/grants/final_merged_enhanced.csv')\n",
    "            self.companies = pd.read_csv('data/companies/synthetic_companies.csv')\n",
    "            print(f\"âœ… Loaded {len(self.grants)} grants and {len(self.companies)} companies\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"âŒ Error loading data: {e}\")\n",
    "            print(\"ðŸ’¡ Make sure data files exist in data/grants/ and data/companies/\")\n",
    "            raise\n",
    "        \n",
    "        # Setup Gemini AI with correct model\n",
    "        self.ai_enabled = False\n",
    "        api_key = os.getenv('GEMINI_API_KEY')\n",
    "        \n",
    "        if GEMINI_AVAILABLE and api_key:\n",
    "            try:\n",
    "                genai.configure(api_key=api_key)\n",
    "                # FIXED: Using the model you have access to\n",
    "                self.model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "                self.ai_enabled = True\n",
    "                print(\"âœ… Gemini AI enabled - Ready for recommendations!\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ AI setup failed: {e}\")\n",
    "        else:\n",
    "            print(\"â„¹ï¸ AI disabled - Add GEMINI_API_KEY to .env file to enable\")\n",
    "        \n",
    "        # Clean and prepare data\n",
    "        self._prepare_data()\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Clean and prepare data for matching\"\"\"\n",
    "        # Ensure numeric columns\n",
    "        if 'estimated_value_amount' in self.grants.columns:\n",
    "            self.grants['estimated_value_amount'] = pd.to_numeric(\n",
    "                self.grants['estimated_value_amount'], errors='coerce'\n",
    "            ).fillna(0)\n",
    "        else:\n",
    "            self.grants['estimated_value_amount'] = 0\n",
    "        \n",
    "        # Lowercase text columns for matching\n",
    "        text_columns = ['target_sectors', 'geographic_scope', 'country', 'eligibility_criteria']\n",
    "        for col in text_columns:\n",
    "            if col in self.grants.columns:\n",
    "                self.grants[col] = self.grants[col].astype(str).str.lower()\n",
    "    \n",
    "    def find_matches(self, company_profile, top_n=5):\n",
    "        \"\"\"\n",
    "        Find best matching grants for a company profile\n",
    "        \n",
    "        Args:\n",
    "            company_profile: Dict with keys: company_name, sector, nationality, \n",
    "                           business_stage, funding_need_usd\n",
    "            top_n: Number of top matches to return\n",
    "        \n",
    "        Returns:\n",
    "            List of match dictionaries sorted by score\n",
    "        \"\"\"\n",
    "        matches = []\n",
    "        \n",
    "        # Calculate match scores for all grants\n",
    "        for _, grant in self.grants.iterrows():\n",
    "            score, breakdown = self._calculate_match_score(company_profile, grant)\n",
    "            \n",
    "            if score > 30:  # Only include reasonable matches\n",
    "                matches.append({\n",
    "                    'program_name': grant.get('program_name', 'Unknown Program'),\n",
    "                    'institution': grant.get('institution_name', 'Unknown Institution'),\n",
    "                    'country': grant.get('country', 'Unknown'),\n",
    "                    'funding_amount': grant.get('estimated_value_amount', 0),\n",
    "                    'match_score': round(score, 1),\n",
    "                    'score_breakdown': breakdown,\n",
    "                    'target_sectors': grant.get('target_sectors', 'General'),\n",
    "                    'website': grant.get('website_url', 'Not available'),\n",
    "                    'repayment_required': grant.get('repayment_required', 'Unknown'),\n",
    "                    'grant_data': grant.to_dict()\n",
    "                })\n",
    "        \n",
    "        # Sort by score and return top N\n",
    "        matches_sorted = sorted(matches, key=lambda x: x['match_score'], reverse=True)[:top_n]\n",
    "        \n",
    "        return matches_sorted\n",
    "    \n",
    "    def _calculate_match_score(self, company, grant):\n",
    "        \"\"\"\n",
    "        Calculate comprehensive match score (0-100)\n",
    "        \n",
    "        Returns:\n",
    "            score: Float between 0-100\n",
    "            breakdown: Dict with score components\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        breakdown = {}\n",
    "        \n",
    "        # 1. Geographic Match (40 points) - Most important\n",
    "        geo_score = self._score_geography(company, grant)\n",
    "        score += geo_score\n",
    "        breakdown['geographic'] = geo_score\n",
    "        \n",
    "        # 2. Sector Match (30 points)\n",
    "        sector_score = self._score_sector(company, grant)\n",
    "        score += sector_score\n",
    "        breakdown['sector'] = sector_score\n",
    "        \n",
    "        # 3. Funding Amount Fit (20 points)\n",
    "        amount_score = self._score_funding_amount(company, grant)\n",
    "        score += amount_score\n",
    "        breakdown['amount_fit'] = amount_score\n",
    "        \n",
    "        # 4. Stage Bonus (10 points)\n",
    "        stage_score = self._score_business_stage(company, grant)\n",
    "        score += stage_score\n",
    "        breakdown['stage'] = stage_score\n",
    "        \n",
    "        return min(100, score), breakdown\n",
    "    \n",
    "    def _score_geography(self, company, grant):\n",
    "        \"\"\"Score geographic eligibility (0-40 points)\"\"\"\n",
    "        company_country = str(company.get('nationality', '')).lower()\n",
    "        grant_scope = str(grant.get('geographic_scope', '')).lower()\n",
    "        grant_country = str(grant.get('country', '')).lower()\n",
    "        \n",
    "        # Global programs get full points\n",
    "        if 'global' in grant_scope:\n",
    "            return 40\n",
    "        \n",
    "        # Exact country match\n",
    "        if company_country in grant_country or company_country in grant_scope:\n",
    "            return 40\n",
    "        \n",
    "        # Regional matches\n",
    "        africa_countries = ['nigeria', 'kenya', 'south africa', 'ghana', 'uganda', 'egypt', \n",
    "                          'tanzania', 'rwanda', 'ethiopia', 'senegal']\n",
    "        \n",
    "        if company_country in africa_countries:\n",
    "            if 'africa' in grant_scope or 'african' in grant_scope:\n",
    "                return 35\n",
    "        \n",
    "        # No geographic match\n",
    "        return 0\n",
    "    \n",
    "    def _score_sector(self, company, grant):\n",
    "        \"\"\"Score sector alignment (0-30 points)\"\"\"\n",
    "        company_sector = str(company.get('sector', '')).lower()\n",
    "        target_sectors = str(grant.get('target_sectors', '')).lower()\n",
    "        \n",
    "        # All sectors accepted\n",
    "        if 'all' in target_sectors or 'general' in target_sectors or 'any' in target_sectors:\n",
    "            return 25\n",
    "        \n",
    "        # Exact sector match\n",
    "        if company_sector in target_sectors:\n",
    "            return 30\n",
    "        \n",
    "        # Partial match (e.g., \"tech\" in \"technology\")\n",
    "        sector_words = company_sector.split()\n",
    "        if any(word in target_sectors for word in sector_words):\n",
    "            return 20\n",
    "        \n",
    "        # No match but not excluded\n",
    "        return 10\n",
    "    \n",
    "    def _score_funding_amount(self, company, grant):\n",
    "        \"\"\"Score funding amount fit (0-20 points)\"\"\"\n",
    "        need = company.get('funding_need_usd', 0)\n",
    "        available = grant.get('estimated_value_amount', 0)\n",
    "        \n",
    "        if available == 0 or need == 0:\n",
    "            return 15  # Unknown amount gets partial credit\n",
    "        \n",
    "        ratio = need / available\n",
    "        \n",
    "        # Perfect fit: need is 10%-200% of available\n",
    "        if 0.1 <= ratio <= 2.0:\n",
    "            return 20\n",
    "        \n",
    "        # Good fit: need is 5%-500% of available\n",
    "        elif 0.05 <= ratio <= 5.0:\n",
    "            return 15\n",
    "        \n",
    "        # Poor fit but not impossible\n",
    "        else:\n",
    "            return 8\n",
    "    \n",
    "    def _score_business_stage(self, company, grant):\n",
    "        \"\"\"Score business stage fit (0-10 points)\"\"\"\n",
    "        stage = str(company.get('business_stage', '')).lower()\n",
    "        \n",
    "        # Most grants are flexible on stage\n",
    "        if stage in ['startup', 'early growth']:\n",
    "            return 10\n",
    "        elif stage == 'idea':\n",
    "            return 8\n",
    "        elif stage in ['growth', 'scale-up']:\n",
    "            return 9\n",
    "        else:\n",
    "            return 7\n",
    "    \n",
    "    def get_ai_recommendation(self, company_profile, match):\n",
    "        \"\"\"\n",
    "        Generate clear, simple AI recommendation using Gemini 2.5 Flash\n",
    "        \n",
    "        Args:\n",
    "            company_profile: Dict with company details\n",
    "            match: Dict with matched grant details\n",
    "        \n",
    "        Returns:\n",
    "            String with AI recommendation\n",
    "        \"\"\"\n",
    "        if not self.ai_enabled:\n",
    "            return \"ðŸ”‘ Add your Gemini API key to .env file to get AI-powered recommendations!\"\n",
    "        \n",
    "        # FIXED: Simple, clear prompt for friendly advice\n",
    "        prompt = f\"\"\"You are a friendly business advisor helping someone who is NOT a finance expert.\n",
    "\n",
    "COMPANY:\n",
    "- Business: {company_profile.get('company_name', 'Startup')}\n",
    "- What they do: {company_profile.get('sector', 'Unknown')}\n",
    "- Location: {company_profile.get('nationality', 'Unknown')}\n",
    "- Stage: {company_profile.get('business_stage', 'Unknown')}\n",
    "- Money needed: ${company_profile.get('funding_need_usd', 0):,}\n",
    "\n",
    "FUNDING MATCH:\n",
    "- Program: {match['program_name']}\n",
    "- Institution: {match['institution']}\n",
    "- Amount: ${match['funding_amount']:,}\n",
    "- Match Score: {match['match_score']}/100\n",
    "\n",
    "Write advice using SIMPLE language that anyone can understand. Include these 4 sections:\n",
    "\n",
    "**WHY THIS WORKS:**\n",
    "Explain in 2-3 simple sentences why this funding fits their business. Use everyday words.\n",
    "\n",
    "**WHAT TO DO NEXT:**\n",
    "Give 3 specific actions they can take today. Use simple words like \"create a budget\" not \"develop financial projections.\"\n",
    "\n",
    "**WATCH OUT FOR:**\n",
    "Mention 1-2 realistic challenges in plain English. Be honest but encouraging.\n",
    "\n",
    "**YOUR CHANCES:**\n",
    "Say \"Excellent\", \"Good\", \"Fair\", or \"Challenging\" and explain why in one sentence.\n",
    "\n",
    "Use everyday words. No jargon. Be encouraging but honest. Keep under 200 words.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.model.generate_content(\n",
    "                prompt,\n",
    "                generation_config={\n",
    "                    'temperature': 0.8,\n",
    "                    'top_p': 0.9,\n",
    "                    'max_output_tokens': 600,\n",
    "                }\n",
    "            )\n",
    "            return response.text\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            if \"quota\" in error_msg.lower() or \"rate\" in error_msg.lower():\n",
    "                return \"âš ï¸ Too many requests. Free tier: 15 requests/minute. Please wait 60 seconds and try again.\"\n",
    "            elif \"404\" in error_msg or \"not found\" in error_msg.lower():\n",
    "                return \"âš ï¸ Model not available. Check available models with verify_gemini.py\"\n",
    "            else:\n",
    "                return f\"âš ï¸ AI temporarily unavailable: {error_msg}\"\n",
    "\n",
    "\n",
    "# Test function\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ImaraFund INTELLIGENT MATCHER - TEST\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize matcher\n",
    "        matcher = IntelligentMatcher()\n",
    "        \n",
    "        # Create test company profile\n",
    "        test_company = {\n",
    "            'company_name': 'Test AgriTech',\n",
    "            'sector': 'Agriculture',\n",
    "            'nationality': 'Nigeria',\n",
    "            'business_stage': 'Startup',\n",
    "            'funding_need_usd': 50000\n",
    "        }\n",
    "        \n",
    "        print(f\"ðŸ¢ Test Company: {test_company['company_name']}\")\n",
    "        print(f\"   Sector: {test_company['sector']}\")\n",
    "        print(f\"   Location: {test_company['nationality']}\")\n",
    "        print(f\"   Need: ${test_company['funding_need_usd']:,}\")\n",
    "        print()\n",
    "        \n",
    "        # Find matches\n",
    "        matches = matcher.find_matches(test_company, top_n=3)\n",
    "        \n",
    "        if matches:\n",
    "            print(f\"âœ… Found {len(matches)} matches\\n\")\n",
    "            \n",
    "            # Display top match\n",
    "            top_match = matches[0]\n",
    "            print(f\"ðŸ† Top match: {top_match['program_name']} ({top_match['match_score']}/100)\")\n",
    "            print(f\"   Institution: {top_match['institution']}\")\n",
    "            print(f\"   Country: {top_match['country']}\")\n",
    "            print(f\"   Amount: ${top_match['funding_amount']:,}\")\n",
    "            print()\n",
    "            \n",
    "            # Get AI recommendation\n",
    "            print(\"ðŸ¤– AI Recommendation:\")\n",
    "            print(\"-\" * 60)\n",
    "            ai_advice = matcher.get_ai_recommendation(test_company, top_match)\n",
    "            print(ai_advice)\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ No matches found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during test: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc60611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CSV DATA AUDIT REPORT\n",
      "==================================================\n",
      "\n",
      "[SUMMARY]\n",
      "Total Rows:    103\n",
      "Total Columns: 63\n",
      "\n",
      "[COLUMN AUDIT]\n",
      "             Column Name Data Type  Missing (Qty)  Missing (%)  Unique Values\n",
      "              program_id    object              0         0.00            103\n",
      "            program_name    object              0         0.00            102\n",
      "        institution_name    object              0         0.00             94\n",
      "                 country    object              0         0.00             71\n",
      "                  region    object              0         0.00             14\n",
      "           currency_code    object              0         0.00             57\n",
      "  estimated_value_amount    object              1         0.97             44\n",
      "      repayment_required    object              0         0.00              6\n",
      "            program_type    object             20        19.42              4\n",
      "          target_sectors    object              0         0.00             59\n",
      "         duration_months    object              0         0.00             19\n",
      "        geographic_scope    object              0         0.00              6\n",
      "    eligibility_criteria    object              0         0.00            102\n",
      "     application_process    object             20        19.42             76\n",
      "             website_url    object             20        19.42             77\n",
      "      last_verified_date    object             20        19.42              1\n",
      "                   notes    object              0         0.00            103\n",
      "    target_beneficiaries    object             40        38.83             19\n",
      "        age_restrictions    object            101        98.06              2\n",
      "            gender_focus    object             99        96.12              1\n",
      "     environmental_focus    object             40        38.83              2\n",
      "        innovation_focus    object             40        38.83              2\n",
      "           digital_focus    object             40        38.83              2\n",
      "            export_focus    object             40        38.83              2\n",
      "       minimum_employees   float64             40        38.83              2\n",
      "       maximum_employees   float64             50        48.54              9\n",
      "         minimum_revenue    object             40        38.83              6\n",
      "         maximum_revenue    object             40        38.83              7\n",
      "     collateral_required    object              0         0.00             27\n",
      "           interest_rate    object              2         1.94             37\n",
      "     grace_period_months   float64             20        19.42              8\n",
      "            success_rate   float64            103       100.00              0\n",
      "     total_beneficiaries   float64            102        99.03              1\n",
      "        year_established   float64             40        38.83             39\n",
      "          funding_source    object             40        38.83             58\n",
      "    application_deadline    object              0         0.00             15\n",
      "   language_requirements    object             40        38.83             29\n",
      "    technical_assistance    object             40        38.83              2\n",
      "    mentorship_available    object             40        38.83              2\n",
      "networking_opportunities    object             40        38.83              2\n",
      "       training_provided    object             40        38.83              2\n",
      "   co_financing_required    object             40        38.83              2\n",
      "                verified    object             20        19.42              2\n",
      "        special_features    object             63        61.17             40\n",
      "          minimum_amount   float64             83        80.58              9\n",
      "          maximum_amount   float64             83        80.58             13\n",
      "           women_focused    object             83        80.58              2\n",
      "           youth_focused    object             83        80.58              2\n",
      "     agriculture_focused    object             83        80.58              2\n",
      "   green_climate_focused    object             83        80.58              2\n",
      "          export_support    object             83        80.58              2\n",
      "   technology_innovation    object             83        80.58              2\n",
      "  co_financing_available    object             83        80.58              2\n",
      "            last_updated    object             83        80.58              1\n",
      "      program_start_date    object             83        80.58             19\n",
      "           contact_email    object             83        80.58             20\n",
      "           contact_phone    object             83        80.58             20\n",
      "        language_support    object             83        80.58              8\n",
      "     digital_application    object             83        80.58              2\n",
      "      guarantee_coverage    object             65        63.11             16\n",
      "       verification_date    object             83        80.58              1\n",
      "     target_demographics    object             83        80.58             13\n",
      "         data_source_url    object             83        80.58             20\n",
      "\n",
      "[NUMERICAL STATISTICS]\n",
      "                     count          mean           std      min      25%  \\\n",
      "minimum_employees     63.0  5.000000e-02  3.800000e-01      0.0      0.0   \n",
      "maximum_employees     53.0  1.817700e+02  1.324200e+02      5.0     50.0   \n",
      "grace_period_months   83.0  5.460000e+00  1.245000e+01      0.0      0.0   \n",
      "success_rate           0.0           NaN           NaN      NaN      NaN   \n",
      "total_beneficiaries    1.0  4.500000e+02           NaN    450.0    450.0   \n",
      "year_established      63.0  1.994370e+03  3.199000e+01   1895.0   1976.0   \n",
      "minimum_amount        20.0  1.111250e+06  4.459263e+06   1000.0   5000.0   \n",
      "maximum_amount        20.0  4.270090e+08  1.329892e+09  10000.0  87500.0   \n",
      "\n",
      "                          50%        75%           max  \n",
      "minimum_employees         0.0        0.0  3.000000e+00  \n",
      "maximum_employees       200.0      250.0  5.000000e+02  \n",
      "grace_period_months       0.0        6.0  7.200000e+01  \n",
      "success_rate              NaN        NaN           NaN  \n",
      "total_beneficiaries     450.0      450.0  4.500000e+02  \n",
      "year_established       2008.0     2017.5  2.026000e+03  \n",
      "minimum_amount         5000.0    20000.0  2.000000e+07  \n",
      "maximum_amount       500000.0  5000000.0  5.000000e+09  \n",
      "\n",
      "[TOP CATEGORICAL VALUES]\n",
      "                         count unique  \\\n",
      "program_id                 103    103   \n",
      "program_name               103    102   \n",
      "institution_name           103     94   \n",
      "country                    103     71   \n",
      "region                     103     14   \n",
      "currency_code              103     57   \n",
      "estimated_value_amount     102     44   \n",
      "repayment_required         103      6   \n",
      "program_type                83      4   \n",
      "target_sectors             103     59   \n",
      "duration_months            103     19   \n",
      "geographic_scope           103      6   \n",
      "eligibility_criteria       103    102   \n",
      "application_process         83     76   \n",
      "website_url                 83     77   \n",
      "last_verified_date          83      1   \n",
      "notes                      103    103   \n",
      "target_beneficiaries        63     19   \n",
      "age_restrictions             2      2   \n",
      "gender_focus                 4      1   \n",
      "environmental_focus         63      2   \n",
      "innovation_focus            63      2   \n",
      "digital_focus               63      2   \n",
      "export_focus                63      2   \n",
      "minimum_revenue             63      6   \n",
      "maximum_revenue             63      7   \n",
      "collateral_required        103     27   \n",
      "interest_rate              101     37   \n",
      "funding_source              63     58   \n",
      "application_deadline       103     15   \n",
      "language_requirements       63     29   \n",
      "technical_assistance        63      2   \n",
      "mentorship_available        63      2   \n",
      "networking_opportunities    63      2   \n",
      "training_provided           63      2   \n",
      "co_financing_required       63      2   \n",
      "verified                    83      2   \n",
      "special_features            40     40   \n",
      "women_focused               20      2   \n",
      "youth_focused               20      2   \n",
      "agriculture_focused         20      2   \n",
      "green_climate_focused       20      2   \n",
      "export_support              20      2   \n",
      "technology_innovation       20      2   \n",
      "co_financing_available      20      2   \n",
      "last_updated                20      1   \n",
      "program_start_date          20     19   \n",
      "contact_email               20     20   \n",
      "contact_phone               20     20   \n",
      "language_support            20      8   \n",
      "digital_application         20      2   \n",
      "guarantee_coverage          38     16   \n",
      "verification_date           20      1   \n",
      "target_demographics         20     13   \n",
      "data_source_url             20     20   \n",
      "\n",
      "                                                                        top  \\\n",
      "program_id                                                     AE_DUBAI_001   \n",
      "program_name                                             Almi Business Loan   \n",
      "institution_name                                       Enterprise Singapore   \n",
      "country                                                       United States   \n",
      "region                                                               Europe   \n",
      "currency_code                                                           USD   \n",
      "estimated_value_amount                                             10000000   \n",
      "repayment_required                                                     True   \n",
      "program_type                                                      Financial   \n",
      "target_sectors                                                          All   \n",
      "duration_months                                                          60   \n",
      "geographic_scope                                                   National   \n",
      "eligibility_criteria          Canadian businesses with 12+ months operation   \n",
      "application_process                     Application via participating banks   \n",
      "website_url                                             https://www.bdc.ca/   \n",
      "last_verified_date                                               2026-02-06   \n",
      "notes                     Up to 1M AED interest-free loans; loan repayme...   \n",
      "target_beneficiaries                                                   SMEs   \n",
      "age_restrictions                                                      18-35   \n",
      "gender_focus                                                          Women   \n",
      "environmental_focus                                                      No   \n",
      "innovation_focus                                                         No   \n",
      "digital_focus                                                            No   \n",
      "export_focus                                                             No   \n",
      "minimum_revenue                                                           0   \n",
      "maximum_revenue                                                           0   \n",
      "collateral_required                                                     Yes   \n",
      "interest_rate                                                      Variable   \n",
      "funding_source                                          Canadian Government   \n",
      "application_deadline                                                Rolling   \n",
      "language_requirements                                               English   \n",
      "technical_assistance                                                    Yes   \n",
      "mentorship_available                                                     No   \n",
      "networking_opportunities                                                Yes   \n",
      "training_provided                                                        No   \n",
      "co_financing_required                                                    No   \n",
      "verified                                                               True   \n",
      "special_features          80% government guarantee; repayment can begin ...   \n",
      "women_focused                                                            No   \n",
      "youth_focused                                                            No   \n",
      "agriculture_focused                                                     Yes   \n",
      "green_climate_focused                                                    No   \n",
      "export_support                                                           No   \n",
      "technology_innovation                                                    No   \n",
      "co_financing_available                                                  Yes   \n",
      "last_updated                                                     2026-02-06   \n",
      "program_start_date                                               2018-01-01   \n",
      "contact_email                                                 bidc@bidc.org   \n",
      "contact_phone                                               +1-246-427-5350   \n",
      "language_support                                                    English   \n",
      "digital_application                                                     Yes   \n",
      "guarantee_coverage                                                       0%   \n",
      "verification_date                                                2026-02-06   \n",
      "target_demographics                                                    SMEs   \n",
      "data_source_url           https://www.morrows.com.au/changes-to-the-sme-...   \n",
      "\n",
      "                         freq  \n",
      "program_id                  1  \n",
      "program_name                2  \n",
      "institution_name            2  \n",
      "country                     5  \n",
      "region                     17  \n",
      "currency_code              15  \n",
      "estimated_value_amount      9  \n",
      "repayment_required         54  \n",
      "program_type               49  \n",
      "target_sectors             29  \n",
      "duration_months            35  \n",
      "geographic_scope           92  \n",
      "eligibility_criteria        2  \n",
      "application_process         5  \n",
      "website_url                 2  \n",
      "last_verified_date         83  \n",
      "notes                       1  \n",
      "target_beneficiaries       29  \n",
      "age_restrictions            1  \n",
      "gender_focus                4  \n",
      "environmental_focus        55  \n",
      "innovation_focus           43  \n",
      "digital_focus              57  \n",
      "export_focus               59  \n",
      "minimum_revenue            57  \n",
      "maximum_revenue            52  \n",
      "collateral_required        29  \n",
      "interest_rate              51  \n",
      "funding_source              2  \n",
      "application_deadline       71  \n",
      "language_requirements      14  \n",
      "technical_assistance       57  \n",
      "mentorship_available       40  \n",
      "networking_opportunities   38  \n",
      "training_provided          32  \n",
      "co_financing_required      44  \n",
      "verified                   63  \n",
      "special_features            1  \n",
      "women_focused              15  \n",
      "youth_focused              16  \n",
      "agriculture_focused        15  \n",
      "green_climate_focused      16  \n",
      "export_support             18  \n",
      "technology_innovation      18  \n",
      "co_financing_available     15  \n",
      "last_updated               20  \n",
      "program_start_date          2  \n",
      "contact_email               1  \n",
      "contact_phone               1  \n",
      "language_support           13  \n",
      "digital_application        12  \n",
      "guarantee_coverage         13  \n",
      "verification_date          20  \n",
      "target_demographics         7  \n",
      "data_source_url             1  \n",
      "\n",
      "==================================================\n",
      "Audit Complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Your specific file path\n",
    "file_path = r\"D:\\D1\\WTF\\FundCompass\\data\\grants\\final_merged_enhanced.csv\"\n",
    "\n",
    "def audit_csv(path):\n",
    "    try:\n",
    "        # 1. Load the data\n",
    "        df = pd.read_csv(path)\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        print(\"CSV DATA AUDIT REPORT\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # 2. Basic Dimensions\n",
    "        rows, cols = df.shape\n",
    "        print(f\"\\n[SUMMARY]\")\n",
    "        print(f\"Total Rows:    {rows}\")\n",
    "        print(f\"Total Columns: {cols}\")\n",
    "\n",
    "        # 3. Column Audit (Names, Types, and Emptiness)\n",
    "        print(f\"\\n[COLUMN AUDIT]\")\n",
    "        \n",
    "        # Calculate missing values\n",
    "        null_counts = df.isnull().sum()\n",
    "        null_percentages = (null_counts / rows) * 100\n",
    "        unique_counts = df.nunique()\n",
    "        \n",
    "        # Combine into a clean audit table\n",
    "        audit_table = pd.DataFrame({\n",
    "            'Column Name': df.columns,\n",
    "            'Data Type': df.dtypes.values,\n",
    "            'Missing (Qty)': null_counts.values,\n",
    "            'Missing (%)': null_percentages.values.round(2),\n",
    "            'Unique Values': unique_counts.values\n",
    "        })\n",
    "        \n",
    "        print(audit_table.to_string(index=False))\n",
    "\n",
    "        # 4. Statistical Summary (Numerical Columns)\n",
    "        # Includes Mean, Min, Max, and Percentiles\n",
    "        if not df.select_dtypes(include=['number']).empty:\n",
    "            print(f\"\\n[NUMERICAL STATISTICS]\")\n",
    "            print(df.describe().T.round(2))\n",
    "        else:\n",
    "            print(\"\\n[NUMERICAL STATISTICS] No numerical columns found.\")\n",
    "\n",
    "        # 5. Categorical Summary (Non-numerical Columns)\n",
    "        categorical_cols = df.select_dtypes(include=['object'])\n",
    "        if not categorical_cols.empty:\n",
    "            print(f\"\\n[TOP CATEGORICAL VALUES]\")\n",
    "            # Show the most frequent value for the first few categorical columns\n",
    "            print(df.describe(include=['object']).T)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Audit Complete.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find the file at {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audit_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acf02a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CSV DATA AUDIT REPORT\n",
      "==================================================\n",
      "\n",
      "[SUMMARY]\n",
      "Total Rows:    50\n",
      "Total Columns: 16\n",
      "\n",
      "[COLUMN AUDIT]\n",
      "           Column Name Data Type  Missing (Qty)  Missing (%)  Unique Values\n",
      "            company_id    object              0          0.0             50\n",
      "          company_name    object              0          0.0             50\n",
      "                sector    object              0          0.0             10\n",
      "           nationality    object              0          0.0             15\n",
      "business_registered_in    object              0          0.0             14\n",
      "           founder_age     int64              0          0.0             25\n",
      "        founder_gender    object              0          0.0              2\n",
      "   business_age_months     int64              0          0.0             35\n",
      "    annual_revenue_usd     int64              0          0.0             10\n",
      "             employees     int64              0          0.0             18\n",
      "        business_stage    object              0          0.0              5\n",
      "      funding_need_usd     int64              0          0.0              8\n",
      "         has_prototype      bool              0          0.0              2\n",
      "      innovation_level    object              0          0.0              3\n",
      "   targets_underserved      bool              0          0.0              2\n",
      "          created_date    object              0          0.0             13\n",
      "\n",
      "[NUMERICAL STATISTICS]\n",
      "                     count        mean         std      min       25%  \\\n",
      "founder_age           50.0       39.86        7.47     25.0      35.0   \n",
      "business_age_months   50.0       32.52       15.94      0.0      19.0   \n",
      "annual_revenue_usd    50.0   930500.00  1562355.38      0.0  100000.0   \n",
      "employees             50.0        8.16       10.52      1.0       2.0   \n",
      "funding_need_usd      50.0  1482000.00  2622790.81  25000.0  250000.0   \n",
      "\n",
      "                          50%         75%         max  \n",
      "founder_age              41.0       44.00        54.0  \n",
      "business_age_months      35.0       43.75        59.0  \n",
      "annual_revenue_usd   200000.0  1000000.00   5000000.0  \n",
      "employees                 4.0        9.75        48.0  \n",
      "funding_need_usd     500000.0  1750000.00  10000000.0  \n",
      "\n",
      "[TOP CATEGORICAL VALUES]\n",
      "                       count unique                         top freq\n",
      "company_id                50     50                     COMP001    1\n",
      "company_name              50     50            SmartSolutions 1    1\n",
      "sector                    50     10                  Healthcare   11\n",
      "nationality               50     15                   Australia    6\n",
      "business_registered_in    50     14                South Africa    7\n",
      "founder_gender            50      2                        male   30\n",
      "business_stage            50      5                      Growth   23\n",
      "innovation_level          50      3                      Medium   25\n",
      "created_date              50     13  2026-02-17T20:24:28.300374    5\n",
      "\n",
      "==================================================\n",
      "Audit Complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Your specific file path\n",
    "file_path = r\"D:\\D1\\WTF\\FundCompass\\data\\companies\\synthetic_companies.csv\"\n",
    "\n",
    "def audit_csv(path):\n",
    "    try:\n",
    "        # 1. Load the data\n",
    "        df = pd.read_csv(path)\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        print(\"CSV DATA AUDIT REPORT\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # 2. Basic Dimensions\n",
    "        rows, cols = df.shape\n",
    "        print(f\"\\n[SUMMARY]\")\n",
    "        print(f\"Total Rows:    {rows}\")\n",
    "        print(f\"Total Columns: {cols}\")\n",
    "\n",
    "        # 3. Column Audit (Names, Types, and Emptiness)\n",
    "        print(f\"\\n[COLUMN AUDIT]\")\n",
    "        \n",
    "        # Calculate missing values\n",
    "        null_counts = df.isnull().sum()\n",
    "        null_percentages = (null_counts / rows) * 100\n",
    "        unique_counts = df.nunique()\n",
    "        \n",
    "        # Combine into a clean audit table\n",
    "        audit_table = pd.DataFrame({\n",
    "            'Column Name': df.columns,\n",
    "            'Data Type': df.dtypes.values,\n",
    "            'Missing (Qty)': null_counts.values,\n",
    "            'Missing (%)': null_percentages.values.round(2),\n",
    "            'Unique Values': unique_counts.values\n",
    "        })\n",
    "        \n",
    "        print(audit_table.to_string(index=False))\n",
    "\n",
    "        # 4. Statistical Summary (Numerical Columns)\n",
    "        # Includes Mean, Min, Max, and Percentiles\n",
    "        if not df.select_dtypes(include=['number']).empty:\n",
    "            print(f\"\\n[NUMERICAL STATISTICS]\")\n",
    "            print(df.describe().T.round(2))\n",
    "        else:\n",
    "            print(\"\\n[NUMERICAL STATISTICS] No numerical columns found.\")\n",
    "\n",
    "        # 5. Categorical Summary (Non-numerical Columns)\n",
    "        categorical_cols = df.select_dtypes(include=['object'])\n",
    "        if not categorical_cols.empty:\n",
    "            print(f\"\\n[TOP CATEGORICAL VALUES]\")\n",
    "            # Show the most frequent value for the first few categorical columns\n",
    "            print(df.describe(include=['object']).T)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Audit Complete.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find the file at {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audit_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ImaraFund Project Setup & Advanced Data Cleaning\n",
    "=================================================\n",
    "This script:\n",
    "1. Creates proper FastAPI folder structure\n",
    "2. Archives old files safely (including Control_V2.ipynb)\n",
    "3. Fixes grants dataset (fills application_url from website_url)\n",
    "4. Performs comprehensive data cleaning for all 63 columns\n",
    "5. Creates .gitignore and project files\n",
    "6. Prepares data for seamless backend integration\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "class ImaraFundSetup:\n",
    "    def __init__(self, project_root: str = r\"D:\\D1\\WTF\\ImaraFund\"):\n",
    "        self.root = Path(project_root)\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"ðŸš€ ImaraFund Project Setup & Advanced Data Cleaning\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"ðŸ“ Project Root: {self.root}\")\n",
    "        print(f\"â° Timestamp: {self.timestamp}\\n\")\n",
    "    \n",
    "    def create_folder_structure(self):\n",
    "        \"\"\"Create comprehensive FastAPI project structure.\"\"\"\n",
    "        print(\"ðŸ“¦ Creating ImaraFund folder structure...\")\n",
    "        \n",
    "        folders = [\n",
    "            \"archive\",\n",
    "            \"app\",\n",
    "            \"app/api\", \n",
    "            \"app/services\",\n",
    "            \"app/core\",\n",
    "            \"data\",\n",
    "            \"data/grants\",\n",
    "            \"data/companies\", \n",
    "            \"data/cleaned\",\n",
    "            \"migrations\",\n",
    "            \"tests\",\n",
    "            \"docs\",\n",
    "            \"static\"\n",
    "        ]\n",
    "        \n",
    "        for folder in folders:\n",
    "            folder_path = self.root / folder\n",
    "            folder_path.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"  âœ“ Created: {folder}\")\n",
    "        \n",
    "        # Create __init__.py files for Python packages\n",
    "        init_files = [\n",
    "            \"app/__init__.py\",\n",
    "            \"app/api/__init__.py\", \n",
    "            \"app/services/__init__.py\",\n",
    "            \"app/core/__init__.py\"\n",
    "        ]\n",
    "        \n",
    "        for init_file in init_files:\n",
    "            init_path = self.root / init_file\n",
    "            init_path.touch(exist_ok=True)\n",
    "        \n",
    "        print(\"âœ… Folder structure created!\\n\")\n",
    "    \n",
    "    def archive_old_files(self):\n",
    "        \"\"\"\n",
    "        Safely move old files and specific notebooks to the archive.\n",
    "        This includes Control_V2.ipynb and any other legacy files.\n",
    "        \"\"\"\n",
    "        print(\"ðŸ“¦ Archiving old files and notebooks...\")\n",
    "        \n",
    "        archive_folder = self.root / \"archive\" / self.timestamp\n",
    "        archive_folder.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Files that MUST STAY in the root (do not move these)\n",
    "        preserve = {\n",
    "            \"setup_imarafund.py\", \"app\", \"data\", \"migrations\", \"archive\", \n",
    "            \".git\", \".env\", \".gitignore\", \"requirements.txt\", \"README.md\",\n",
    "            \"venv\", \".venv\", \"ENV\"  # Preserve virtual environments\n",
    "        }\n",
    "        \n",
    "        archived_count = 0\n",
    "        special_files_archived = []  # Track important files like notebooks\n",
    "        \n",
    "        for item in self.root.iterdir():\n",
    "            # If it's NOT in the preserve list, move it to archive\n",
    "            if item.name not in preserve:\n",
    "                try:\n",
    "                    dest = archive_folder / item.name\n",
    "                    \n",
    "                    # Move the file or directory\n",
    "                    if item.is_file():\n",
    "                        shutil.move(str(item), str(dest))\n",
    "                    elif item.is_dir():\n",
    "                        shutil.move(str(item), str(dest))\n",
    "                    \n",
    "                    # Special tracking for notebooks and important files\n",
    "                    if item.suffix in ['.ipynb', '.py', '.csv', '.json']:\n",
    "                        special_files_archived.append(item.name)\n",
    "                        print(f\"  âœ“ Archived (Important): {item.name}\")\n",
    "                    else:\n",
    "                        print(f\"  âœ“ Archived: {item.name}\")\n",
    "                    \n",
    "                    archived_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  âš  Could not archive {item.name}: {e}\")\n",
    "        \n",
    "        # **SPECIAL CONFIRMATION for Control_V2.ipynb**\n",
    "        if \"Control_V2.ipynb\" in special_files_archived:\n",
    "            print(f\"\\n  ðŸŽ¯ SPECIAL CONFIRMATION: Control_V2.ipynb successfully archived!\")\n",
    "            print(f\"     ðŸ“ Location: archive/{self.timestamp}/Control_V2.ipynb\")\n",
    "            print(f\"     âœ¨ Your notebook is safe and can be restored anytime!\")\n",
    "        \n",
    "        # Summary of special files\n",
    "        if special_files_archived:\n",
    "            print(f\"\\n  ðŸ“‹ Important files archived ({len(special_files_archived)}):\")\n",
    "            for file in special_files_archived:\n",
    "                print(f\"     â€¢ {file}\")\n",
    "        \n",
    "        print(f\"\\nâœ… Archived {archived_count} items to archive/{self.timestamp}\\n\")\n",
    "    \n",
    "    def find_grants_csv(self):\n",
    "        \"\"\"Locate the grants CSV file in various possible locations.\"\"\"\n",
    "        possible_paths = [\n",
    "            self.root / \"data\" / \"grants\" / \"final_merged_enhanced.csv\",\n",
    "            self.root / \"final_merged_enhanced.csv\",\n",
    "            self.root / \"grants.csv\"\n",
    "        ]\n",
    "        \n",
    "        # Search in archive directories as fallback\n",
    "        archive_base = self.root / \"archive\"\n",
    "        if archive_base.exists():\n",
    "            for archive_dir in sorted(archive_base.glob(\"*\"), reverse=True):\n",
    "                if archive_dir.is_dir():\n",
    "                    possible_archive_path = archive_dir / \"final_merged_enhanced.csv\"\n",
    "                    if possible_archive_path.exists():\n",
    "                        return possible_archive_path\n",
    "        \n",
    "        for path in possible_paths:\n",
    "            if path.exists():\n",
    "                return path\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def clean_currency_value(self, value):\n",
    "        \"\"\"Clean currency values removing symbols and converting to float.\"\"\"\n",
    "        if pd.isna(value) or value == \"\" or value is None:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            cleaned = str(value).replace(\"$\", \"\").replace(\",\", \"\").replace(\" \", \"\").strip()\n",
    "            if not cleaned or cleaned.lower() in ['nan', 'null', 'none', '']:\n",
    "                return None\n",
    "            return float(cleaned)\n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "    \n",
    "    def standardize_boolean(self, value):\n",
    "        \"\"\"Convert various boolean representations to proper boolean.\"\"\"\n",
    "        if pd.isna(value) or value == \"\" or value is None:\n",
    "            return False\n",
    "        \n",
    "        if isinstance(value, bool):\n",
    "            return value\n",
    "        \n",
    "        str_val = str(value).lower().strip()\n",
    "        return str_val in [\"true\", \"yes\", \"1\", \"y\", \"on\", \"t\"]\n",
    "    \n",
    "    def clean_text_field(self, value, max_length=None):\n",
    "        \"\"\"Clean text fields with length limits.\"\"\"\n",
    "        if pd.isna(value) or value is None:\n",
    "            return None\n",
    "        \n",
    "        result = str(value).strip()\n",
    "        if not result or result.lower() in ['nan', 'null', 'none']:\n",
    "            return None\n",
    "        \n",
    "        if max_length and len(result) > max_length:\n",
    "            result = result[:max_length-3] + \"...\"\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def fix_grants_dataset(self):\n",
    "        \"\"\"Comprehensive grants dataset cleaning and fixing.\"\"\"\n",
    "        print(\"ðŸ”§ Fixing and cleaning grants dataset...\")\n",
    "        \n",
    "        grants_file = self.find_grants_csv()\n",
    "        if not grants_file:\n",
    "            print(\"  âš  Grants CSV file not found. Please place 'final_merged_enhanced.csv' in:\")\n",
    "            print(\"     â€¢ Project root: D:\\\\D1\\\\WTF\\\\ImaraFund\\\\\")\n",
    "            print(\"     â€¢ Or in: data/grants/\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"  ðŸ“„ Found grants file: {grants_file}\")\n",
    "        \n",
    "        # Load dataset with encoding fallback\n",
    "        try:\n",
    "            df = pd.read_csv(grants_file, encoding='utf-8')\n",
    "            print(f\"  âœ“ Loaded with UTF-8 encoding\")\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                df = pd.read_csv(grants_file, encoding='latin-1')\n",
    "                print(f\"  âœ“ Loaded with Latin-1 encoding\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Could not read CSV file: {e}\")\n",
    "                return None\n",
    "        \n",
    "        print(f\"  ðŸ“Š Loaded {len(df)} grants with {len(df.columns)} columns\")\n",
    "        \n",
    "        # **CRITICAL FIX: Application URL Column**\n",
    "        print(\"\\n  ðŸ”— Fixing application URL column...\")\n",
    "        \n",
    "        # Find the correct column names (case-insensitive)\n",
    "        app_url_col = None\n",
    "        website_col = None\n",
    "        \n",
    "        for col in df.columns:\n",
    "            col_lower = col.lower()\n",
    "            if 'application' in col_lower and ('url' in col_lower or 'link' in col_lower):\n",
    "                app_url_col = col\n",
    "                print(f\"    â„¹ Found application URL column: {col}\")\n",
    "            elif 'website' in col_lower and ('url' in col_lower or 'link' in col_lower):\n",
    "                website_col = col\n",
    "                print(f\"    â„¹ Found website URL column: {col}\")\n",
    "        \n",
    "        # Create application_url if it doesn't exist\n",
    "        if not app_url_col:\n",
    "            app_url_col = 'application_url'\n",
    "            df[app_url_col] = None\n",
    "            print(f\"    â„¹ Created new column: {app_url_col}\")\n",
    "        \n",
    "        # Count missing values before fix\n",
    "        missing_before = df[app_url_col].isna().sum()\n",
    "        print(f\"    ðŸ“‰ Missing {app_url_col} before fix: {missing_before} ({missing_before/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        if website_col and website_col in df.columns:\n",
    "            missing_website = df[website_col].isna().sum()\n",
    "            print(f\"    ðŸ“‰ Missing {website_col}: {missing_website} ({missing_website/len(df)*100:.1f}%)\")\n",
    "            \n",
    "            # Use pandas combine_first for elegant filling\n",
    "            df[app_url_col] = df[app_url_col].combine_first(df[website_col])\n",
    "            \n",
    "            missing_after = df[app_url_col].isna().sum()\n",
    "            filled_count = missing_before - missing_after\n",
    "            print(f\"    âœ… Filled {filled_count} missing {app_url_col} values from {website_col}\")\n",
    "            print(f\"    âœ… Missing {app_url_col} after fix: {missing_after} ({missing_after/len(df)*100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"    âš  No website URL column found to fill from\")\n",
    "        \n",
    "        # **COMPREHENSIVE DATA CLEANING**\n",
    "        print(\"\\n  ðŸ§¹ Performing comprehensive data cleaning...\")\n",
    "        \n",
    "        # 1. Clean currency/numeric columns\n",
    "        currency_columns = [\n",
    "            'estimated_value_amount', 'minimum_amount', 'maximum_amount', \n",
    "            'minimum_revenue', 'maximum_revenue', 'success_rate'\n",
    "        ]\n",
    "        \n",
    "        cleaned_currency = 0\n",
    "        for col in currency_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(self.clean_currency_value)\n",
    "                cleaned_currency += 1\n",
    "        \n",
    "        print(f\"    âœ“ Cleaned {cleaned_currency} currency columns\")\n",
    "        \n",
    "        # 2. Standardize boolean columns\n",
    "        boolean_columns = [\n",
    "            'repayment_required', 'environmental_focus', 'innovation_focus',\n",
    "            'digital_focus', 'export_focus', 'women_focused', 'youth_focused',\n",
    "            'agriculture_focused', 'green_climate_focused', 'technical_assistance',\n",
    "            'mentorship_available', 'networking_opportunities', 'training_provided',\n",
    "            'co_financing_required', 'co_financing_available', 'export_support',\n",
    "            'technology_innovation', 'digital_application', 'verified'\n",
    "        ]\n",
    "        \n",
    "        cleaned_bools = 0\n",
    "        for col in boolean_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(self.standardize_boolean)\n",
    "                cleaned_bools += 1\n",
    "        \n",
    "        print(f\"    âœ“ Standardized {cleaned_bools} boolean columns\")\n",
    "        \n",
    "        # 3. Clean text fields with length limits\n",
    "        text_fields = {\n",
    "            'program_name': 500,\n",
    "            'institution_name': 500, \n",
    "            'eligibility_criteria': None,\n",
    "            'application_process': None,\n",
    "            'notes': None,\n",
    "            'special_features': None,\n",
    "            'program_description': None\n",
    "        }\n",
    "        \n",
    "        cleaned_text = 0\n",
    "        for col, max_len in text_fields.items():\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(lambda x: self.clean_text_field(x, max_len))\n",
    "                cleaned_text += 1\n",
    "        \n",
    "        print(f\"    âœ“ Cleaned {cleaned_text} text columns\")\n",
    "        \n",
    "        # 4. Handle integer columns\n",
    "        int_columns = ['duration_months', 'minimum_employees', 'maximum_employees', \n",
    "                      'grace_period_months', 'total_beneficiaries', 'year_established']\n",
    "        \n",
    "        cleaned_ints = 0\n",
    "        for col in int_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')\n",
    "                cleaned_ints += 1\n",
    "        \n",
    "        print(f\"    âœ“ Cleaned {cleaned_ints} integer columns\")\n",
    "        \n",
    "        # **SAVE CLEANED DATASET**\n",
    "        output_path = self.root / \"data\" / \"grants\" / \"final_merged_enhanced_clean.csv\"\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        print(f\"\\n  ðŸ’¾ Cleaned dataset saved to: {output_path.relative_to(self.root)}\")\n",
    "        \n",
    "        # Generate comprehensive data quality report\n",
    "        report_path = output_path.parent / \"data_quality_report.txt\"\n",
    "        self.generate_data_report(df, report_path)\n",
    "        \n",
    "        # Create a backup of original if different location\n",
    "        if grants_file != output_path:\n",
    "            backup_path = self.root / \"archive\" / self.timestamp / grants_file.name\n",
    "            backup_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(str(grants_file), str(backup_path))\n",
    "            print(f\"  ðŸ’¾ Original backed up to: archive/{self.timestamp}/{grants_file.name}\")\n",
    "        \n",
    "        print(\"âœ… Grants dataset cleaning completed!\\n\")\n",
    "        return df\n",
    "    \n",
    "    def generate_data_report(self, df, output_path):\n",
    "        \"\"\"Generate detailed data quality report.\"\"\"\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(\"IMARAFUND GRANTS - DATA QUALITY REPORT\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(f\"ðŸ“Š DATASET OVERVIEW\\n\")\n",
    "            f.write(f\"Total Records: {len(df):,}\\n\")\n",
    "            f.write(f\"Total Columns: {len(df.columns)}\\n\\n\")\n",
    "            \n",
    "            f.write(\"ðŸ” MISSING DATA ANALYSIS\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "            missing = df.isna().sum()\n",
    "            missing_pct = (missing / len(df) * 100).round(2)\n",
    "            \n",
    "            # Sort by missing percentage (descending)\n",
    "            missing_sorted = missing_pct.sort_values(ascending=False)\n",
    "            \n",
    "            for col in missing_sorted.index:\n",
    "                if missing[col] > 0:\n",
    "                    f.write(f\"  {col:<35}: {missing[col]:>4} ({missing_pct[col]:>6.1f}%)\\n\")\n",
    "            \n",
    "            f.write(f\"\\nðŸ“ˆ DATA COMPLETENESS SUMMARY\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "            f.write(f\"Columns with 0% missing:    {(missing == 0).sum():>3}\\n\")\n",
    "            f.write(f\"Columns with <10% missing:  {(missing_pct < 10).sum():>3}\\n\")\n",
    "            f.write(f\"Columns with 10-50% missing: {((missing_pct >= 10) & (missing_pct < 50)).sum():>3}\\n\")\n",
    "            f.write(f\"Columns with >50% missing:  {(missing_pct >= 50).sum():>3}\\n\")\n",
    "            \n",
    "            # Key columns for matching algorithm\n",
    "            f.write(f\"\\nðŸŽ¯ KEY MATCHING COLUMNS STATUS\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "            key_columns = {\n",
    "                'country': 'Geography (40% weight)',\n",
    "                'geographic_scope': 'Geography (40% weight)',\n",
    "                'target_sectors': 'Sector (30% weight)',\n",
    "                'estimated_value_amount': 'Funding (20% weight)',\n",
    "                'program_name': 'Core identification',\n",
    "                'institution_name': 'Core identification',\n",
    "                'application_url': 'Application process (FIXED)',\n",
    "                'website_url': 'Contact information'\n",
    "            }\n",
    "            \n",
    "            for col, description in key_columns.items():\n",
    "                if col in df.columns:\n",
    "                    miss = missing[col]\n",
    "                    miss_pct = missing_pct[col]\n",
    "                    status = \"âœ… Excellent\" if miss_pct < 5 else \"âœ“ Good\" if miss_pct < 20 else \"âš  Needs attention\"\n",
    "                    f.write(f\"  {col:<25}: {status:<20} {miss:>3} missing ({miss_pct:>5.1f}%) - {description}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "        \n",
    "        print(f\"  ðŸ“Š Data quality report saved: {output_path.relative_to(self.root)}\")\n",
    "    \n",
    "    def create_project_files(self):\n",
    "        \"\"\"Create essential project files.\"\"\"\n",
    "        print(\"ðŸ“ Creating project configuration files...\")\n",
    "        \n",
    "        # .gitignore\n",
    "        gitignore_content = \"\"\"# ImaraFund .gitignore\n",
    "\n",
    "# Archive folder - contains old/backup files  \n",
    "archive/\n",
    "\n",
    "# Python\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*$py.class\n",
    "*.so\n",
    ".Python\n",
    "build/\n",
    "develop-eggs/\n",
    "dist/\n",
    "downloads/\n",
    "eggs/\n",
    ".eggs/\n",
    "lib/\n",
    "lib64/\n",
    "parts/\n",
    "sdist/\n",
    "var/\n",
    "wheels/\n",
    "*.egg-info/\n",
    ".installed.cfg\n",
    "*.egg\n",
    "\n",
    "# Virtual Environment\n",
    "venv/\n",
    "ENV/\n",
    "env/\n",
    ".venv/\n",
    "\n",
    "# IDEs\n",
    ".vscode/\n",
    ".idea/\n",
    "*.swp\n",
    "*.swo\n",
    "*~\n",
    "\n",
    "# Jupyter Notebooks (archived versions)\n",
    ".ipynb_checkpoints/\n",
    "*-checkpoint.ipynb\n",
    "\n",
    "# Environment variables\n",
    ".env\n",
    ".env.local\n",
    ".env.*.local\n",
    "\n",
    "# Database\n",
    "*.db\n",
    "*.sqlite\n",
    "*.sqlite3\n",
    "imarafund.db\n",
    "\n",
    "# Logs\n",
    "*.log\n",
    "logs/\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# Testing\n",
    ".pytest_cache/\n",
    ".coverage\n",
    "htmlcov/\n",
    ".tox/\n",
    "\n",
    "# Temporary files\n",
    "*.tmp\n",
    "*.temp\n",
    "~$*\n",
    "\"\"\"\n",
    "        \n",
    "        with open(self.root / \".gitignore\", 'w', encoding='utf-8') as f:\n",
    "            f.write(gitignore_content)\n",
    "        print(\"  âœ“ Created: .gitignore\")\n",
    "        \n",
    "        # requirements.txt\n",
    "        requirements = \"\"\"# ImaraFund Backend Requirements\n",
    "\n",
    "# FastAPI Framework\n",
    "fastapi==0.109.0\n",
    "uvicorn[standard]==0.27.0\n",
    "\n",
    "# Database\n",
    "sqlalchemy==2.0.25\n",
    "alembic==1.13.1\n",
    "\n",
    "# Data Validation  \n",
    "pydantic==2.5.3\n",
    "pydantic-settings==2.1.0\n",
    "\n",
    "# Environment\n",
    "python-dotenv==1.0.0\n",
    "\n",
    "# AI Integration (Your Gemini 2.5 Flash)\n",
    "google-generativeai==0.3.2\n",
    "\n",
    "# Database Drivers\n",
    "psycopg2-binary==2.9.9\n",
    "aiosqlite==0.19.0\n",
    "\n",
    "# Data Processing (Your IntelligentMatcher dependencies)\n",
    "pandas==2.1.4\n",
    "numpy==1.26.3\n",
    "\n",
    "# API Utilities\n",
    "python-multipart==0.0.6\n",
    "\n",
    "# Testing\n",
    "pytest==7.4.3\n",
    "httpx==0.25.2\n",
    "\n",
    "# Development\n",
    "black==23.12.1\n",
    "\"\"\"\n",
    "        \n",
    "        with open(self.root / \"requirements.txt\", 'w', encoding='utf-8') as f:\n",
    "            f.write(requirements)\n",
    "        print(\"  âœ“ Created: requirements.txt\")\n",
    "        \n",
    "        # .env.example\n",
    "        env_example = \"\"\"# ImaraFund Environment Configuration\n",
    "\n",
    "# Database\n",
    "DATABASE_URL=sqlite:///./imarafund.db\n",
    "\n",
    "# API Configuration  \n",
    "PROJECT_NAME=ImaraFund\n",
    "API_V1_PREFIX=/api/v1\n",
    "DEBUG=True\n",
    "\n",
    "# AI Configuration - Get your key from: https://makersuite.google.com/app/apikey\n",
    "GEMINI_API_KEY=your_gemini_api_key_here\n",
    "\n",
    "# Matching Algorithm Weights (Your proven 40/30/20/10 system)\n",
    "GEOGRAPHY_WEIGHT=0.40\n",
    "SECTOR_WEIGHT=0.30\n",
    "FUNDING_WEIGHT=0.20\n",
    "STAGE_WEIGHT=0.10\n",
    "\"\"\"\n",
    "        \n",
    "        with open(self.root / \".env.example\", 'w', encoding='utf-8') as f:\n",
    "            f.write(env_example)\n",
    "        print(\"  âœ“ Created: .env.example\")\n",
    "        \n",
    "        print(\"âœ… Project files created!\\n\")\n",
    "    \n",
    "    def run_setup(self):\n",
    "        \"\"\"Execute complete setup process.\"\"\"\n",
    "        print(\"ðŸŽ¯ Starting ImaraFund complete setup...\\n\")\n",
    "        \n",
    "        # Step 1: Create folder structure\n",
    "        self.create_folder_structure()\n",
    "        \n",
    "        # Step 2: Create project files\n",
    "        self.create_project_files()\n",
    "        \n",
    "        # Step 3: Archive old files (with confirmation)\n",
    "        print(\"=\" * 80)\n",
    "        print(\"ARCHIVE CONFIRMATION\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"The following will be moved to archive/:\")\n",
    "        print(\"  â€¢ Control_V2.ipynb (your notebook)\")\n",
    "        print(\"  â€¢ Any other files not in the new structure\")\n",
    "        print(\"  â€¢ Old CSV files and scripts\")\n",
    "        print(\"\\nThese files will be SAFE in archive/ and can be restored anytime.\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        response = input(\"\\nðŸ“¦ Proceed with archiving? (y/n): \").lower()\n",
    "        if response == 'y':\n",
    "            self.archive_old_files()\n",
    "        else:\n",
    "            print(\"â­ï¸ Skipping archive step. You can run this script again later.\\n\")\n",
    "        \n",
    "        # Step 4: Fix and clean grants dataset\n",
    "        print(\"=\" * 80)\n",
    "        print(\"DATA CLEANING\")\n",
    "        print(\"=\" * 80)\n",
    "        df = self.fix_grants_dataset()\n",
    "        \n",
    "        # Final summary\n",
    "        print(\"=\" * 80)\n",
    "        print(\"âœ… IMARAFUND SETUP COMPLETE!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\nðŸ“‹ What was accomplished:\")\n",
    "        print(\"  âœ“ Created professional FastAPI folder structure\")\n",
    "        print(\"  âœ“ Generated .gitignore, requirements.txt, .env.example\")\n",
    "        if response == 'y':\n",
    "            print(\"  âœ“ Safely archived old files (including Control_V2.ipynb)\")\n",
    "        print(\"  âœ“ Fixed grants dataset (filled application_url from website_url)\")\n",
    "        print(\"  âœ“ Performed comprehensive data cleaning\")\n",
    "        print(\"  âœ“ Generated detailed data quality report\")\n",
    "        \n",
    "        print(\"\\nðŸ“‹ Next Steps:\")\n",
    "        print(\"  1. Review cleaned dataset: data/grants/final_merged_enhanced_clean.csv\")\n",
    "        print(\"  2. Review data quality: data/grants/data_quality_report.txt\")\n",
    "        print(\"  3. Copy companies CSV to: data/companies/synthetic_companies.csv\")\n",
    "        print(\"  4. Copy .env.example to .env and add your GEMINI_API_KEY\")\n",
    "        print(\"  5. Install dependencies: pip install -r requirements.txt\")\n",
    "        print(\"  6. Run migration: python migrations/migration_script.py\")\n",
    "        print(\"  7. Start API: uvicorn app.main:app --reload\")\n",
    "        print(\"  8. Visit: http://localhost:8000/docs\")\n",
    "        \n",
    "        if response == 'y':\n",
    "            print(\"\\nðŸ’¡ Your original work is safely stored in:\")\n",
    "            print(f\"   archive/{self.timestamp}/Control_V2.ipynb\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup = ImaraFundSetup()\n",
    "    setup.run_setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b91da4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸš€ ImaraFund Project Structure Setup (Part 1)\n",
      "======================================================================\n",
      "ðŸ“ Project Root: D:\\D1\\WTF\\ImaraFund\n",
      "â° Timestamp: 20260219_200425\n",
      "\n",
      "ðŸŽ¯ Starting structure-only setup...\n",
      "\n",
      "ðŸ“¦ Creating ImaraFund folder structure...\n",
      "  âœ“ Created: archive/\n",
      "  âœ“ Created: app/\n",
      "  âœ“ Created: app/api/\n",
      "  âœ“ Created: app/services/\n",
      "  âœ“ Created: app/core/\n",
      "  âœ“ Created: data/\n",
      "  âœ“ Created: data/grants/\n",
      "  âœ“ Created: data/companies/\n",
      "  âœ“ Created: data/cleaned/\n",
      "  âœ“ Created: migrations/\n",
      "  âœ“ Created: tests/\n",
      "  âœ“ Created: docs/\n",
      "  âœ“ Created: static/\n",
      "âœ… Folder structure created!\n",
      "\n",
      "ðŸ“ Creating .gitignore...\n",
      "  âœ“ Created: .gitignore\n",
      "âœ… .gitignore created!\n",
      "\n",
      "ðŸ“ Creating project files...\n",
      "  âœ“ Created: requirements.txt\n",
      "  âœ“ Created: .env.example\n",
      "  âœ“ Created: README.md\n",
      "âœ… Project files created!\n",
      "\n",
      "======================================================================\n",
      "ARCHIVE CONFIRMATION\n",
      "======================================================================\n",
      "Archive old files while keeping Control_V2.ipynb in root?\n",
      "(Your notebook will be safe - only other files will be moved)\n",
      "======================================================================\n",
      "ðŸ“¦ Archiving old files...\n",
      "  â„¹ï¸  Preserving Control_V2.ipynb (you're working from it)\n",
      "  âœ“ Archived: .qodo\n",
      "  âœ“ Archived (Important): app.py\n",
      "  âœ“ Archived: APPENDING_TOOL_GUIDE.md\n",
      "  âœ“ Archived (Important): append_sme_programs.py\n",
      "  âš  Could not archive Archive: [('D:\\\\D1\\\\WTF\\\\ImaraFund\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\.qodo\\\\agents', 'D:\\\\D1\\\\WTF\\\\ImaraFund\\\\archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\.qodo\\\\agents', \"[WinError 206] The filename or extension is too long: 'D:\\\\\\\\D1\\\\\\\\WTF\\\\\\\\ImaraFund\\\\\\\\archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\.qodo\\\\\\\\agents'\"), ('D:\\\\D1\\\\WTF\\\\ImaraFund\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\.qodo\\\\workflows', 'D:\\\\D1\\\\WTF\\\\ImaraFund\\\\archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\.qodo\\\\workflows', \"[WinError 206] The filename or extension is too long: 'D:\\\\\\\\D1\\\\\\\\WTF\\\\\\\\ImaraFund\\\\\\\\archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\.qodo\\\\\\\\workflows'\"), ('D:\\\\D1\\\\WTF\\\\ImaraFund\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425', 'D:\\\\D1\\\\WTF\\\\ImaraFund\\\\archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425\\\\Archive\\\\20260219_200425', \"[WinError 206] The filename or extension is too long: 'D:\\\\\\\\D1\\\\\\\\WTF\\\\\\\\ImaraFund\\\\\\\\archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425\\\\\\\\Archive\\\\\\\\20260219_200425'\")]\n",
      "  âœ“ Archived (Important): auditor.ipynb\n",
      "  âœ“ Archived (Important): control.ipynb\n",
      "  âœ“ Archived (Important): final_merged_enhanced_STATISTICS.json\n",
      "  âœ“ Archived: MERGED_DATASET_FEATURES.md\n",
      "  âœ“ Archived: my-file.txt\n",
      "  âœ“ Archived: notebooks\n",
      "  âœ“ Archived: PDF's\n",
      "  âœ“ Archived: pdf.pdf\n",
      "  âœ“ Archived (Important): setup_fundflow_project.py\n",
      "  âœ“ Archived: src\n",
      "\n",
      "  ðŸ“‹ Important files safely archived:\n",
      "     â€¢ app.py\n",
      "     â€¢ append_sme_programs.py\n",
      "     â€¢ auditor.ipynb\n",
      "     â€¢ control.ipynb\n",
      "     â€¢ final_merged_enhanced_STATISTICS.json\n",
      "     â€¢ setup_fundflow_project.py\n",
      "\n",
      "âœ… Archived 14 items to archive/20260219_200425\n",
      "âœ… Control_V2.ipynb preserved in root\n",
      "\n",
      "======================================================================\n",
      "âœ… IMARAFUND STRUCTURE SETUP COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "ðŸ“‹ Created:\n",
      "  âœ“ FastAPI folder structure\n",
      "  âœ“ .gitignore (archive/ ignored)\n",
      "  âœ“ requirements.txt\n",
      "  âœ“ .env.example\n",
      "  âœ“ README.md\n",
      "  âœ“ Old files archived to: archive/20260219_200425/\n",
      "  âœ“ Control_V2.ipynb kept in root\n",
      "\n",
      "ðŸ“‚ Your clean structure:\n",
      "  ImaraFund/\n",
      "  â”œâ”€â”€ Control_V2.ipynb     (your notebook)\n",
      "  â”œâ”€â”€ app/                 (backend code goes here)\n",
      "  â”œâ”€â”€ data/                (CSV files go here)\n",
      "  â”œâ”€â”€ migrations/          (migration scripts)\n",
      "  â”œâ”€â”€ archive/             (old files, git-ignored)\n",
      "  â”œâ”€â”€ .gitignore\n",
      "  â”œâ”€â”€ requirements.txt\n",
      "  â””â”€â”€ README.md\n",
      "\n",
      "ðŸ“‹ Next Steps:\n",
      "  1. Continue working from Control_V2.ipynb\n",
      "  2. When ready, request the 10-step backend implementation\n",
      "  3. Copy backend files into the app/ structure\n",
      "  4. Place your CSV files in data/grants/ and data/companies/\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ImaraFund Project Structure Setup - Part 1 Only\n",
    "===============================================\n",
    "This script ONLY:\n",
    "1. Creates FastAPI folder structure\n",
    "2. Archives old files (preserves Control_V2.ipynb)\n",
    "3. Creates .gitignore file\n",
    "4. Creates essential project files (requirements.txt, .env.example, README.md)\n",
    "\n",
    "NO data cleaning. NO backend code. Just the skeleton structure.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class ImaraFundStructureSetup:\n",
    "    def __init__(self, project_root: str = r\"D:\\D1\\WTF\\ImaraFund\"):\n",
    "        self.root = Path(project_root)\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(\"ðŸš€ ImaraFund Project Structure Setup (Part 1)\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"ðŸ“ Project Root: {self.root}\")\n",
    "        print(f\"â° Timestamp: {self.timestamp}\\n\")\n",
    "    \n",
    "    def create_folder_structure(self):\n",
    "        \"\"\"Create FastAPI project folder structure.\"\"\"\n",
    "        print(\"ðŸ“¦ Creating ImaraFund folder structure...\")\n",
    "        \n",
    "        folders = [\n",
    "            \"archive\",\n",
    "            \"app\",\n",
    "            \"app/api\", \n",
    "            \"app/services\",\n",
    "            \"app/core\",\n",
    "            \"data\",\n",
    "            \"data/grants\",\n",
    "            \"data/companies\", \n",
    "            \"data/cleaned\",\n",
    "            \"migrations\",\n",
    "            \"tests\",\n",
    "            \"docs\",\n",
    "            \"static\"\n",
    "        ]\n",
    "        \n",
    "        for folder in folders:\n",
    "            folder_path = self.root / folder\n",
    "            folder_path.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"  âœ“ Created: {folder}/\")\n",
    "        \n",
    "        # Create __init__.py files for Python packages\n",
    "        init_files = [\n",
    "            \"app/__init__.py\",\n",
    "            \"app/api/__init__.py\", \n",
    "            \"app/services/__init__.py\",\n",
    "            \"app/core/__init__.py\"\n",
    "        ]\n",
    "        \n",
    "        for init_file in init_files:\n",
    "            init_path = self.root / init_file\n",
    "            init_path.touch(exist_ok=True)\n",
    "        \n",
    "        print(\"âœ… Folder structure created!\\n\")\n",
    "    \n",
    "    def create_gitignore(self):\n",
    "        \"\"\"Create comprehensive .gitignore file.\"\"\"\n",
    "        print(\"ðŸ“ Creating .gitignore...\")\n",
    "        \n",
    "        gitignore_content = \"\"\"# ImaraFund .gitignore\n",
    "\n",
    "# Archive folder - contains old/backup files  \n",
    "archive/\n",
    "\n",
    "# Python\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*$py.class\n",
    "*.so\n",
    ".Python\n",
    "build/\n",
    "develop-eggs/\n",
    "dist/\n",
    "downloads/\n",
    "eggs/\n",
    ".eggs/\n",
    "lib/\n",
    "lib64/\n",
    "parts/\n",
    "sdist/\n",
    "var/\n",
    "wheels/\n",
    "*.egg-info/\n",
    ".installed.cfg\n",
    "*.egg\n",
    "\n",
    "# Virtual Environment\n",
    "venv/\n",
    "ENV/\n",
    "env/\n",
    ".venv/\n",
    "\n",
    "# IDEs\n",
    ".vscode/\n",
    ".idea/\n",
    "*.swp\n",
    "*.swo\n",
    "*~\n",
    "\n",
    "# Jupyter Notebooks\n",
    ".ipynb_checkpoints/\n",
    "*-checkpoint.ipynb\n",
    "\n",
    "# Environment variables\n",
    ".env\n",
    ".env.local\n",
    ".env.*.local\n",
    "\n",
    "# Database\n",
    "*.db\n",
    "*.sqlite\n",
    "*.sqlite3\n",
    "imarafund.db\n",
    "\n",
    "# Logs\n",
    "*.log\n",
    "logs/\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# Testing\n",
    ".pytest_cache/\n",
    ".coverage\n",
    "htmlcov/\n",
    ".tox/\n",
    "\n",
    "# Temporary files\n",
    "*.tmp\n",
    "*.temp\n",
    "~$*\n",
    "\"\"\"\n",
    "        \n",
    "        gitignore_path = self.root / \".gitignore\"\n",
    "        with open(gitignore_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(gitignore_content)\n",
    "        \n",
    "        print(f\"  âœ“ Created: .gitignore\")\n",
    "        print(\"âœ… .gitignore created!\\n\")\n",
    "    \n",
    "    def create_project_files(self):\n",
    "        \"\"\"Create essential project files.\"\"\"\n",
    "        print(\"ðŸ“ Creating project files...\")\n",
    "        \n",
    "        # requirements.txt\n",
    "        requirements = \"\"\"# ImaraFund Backend Requirements\n",
    "\n",
    "# FastAPI Framework\n",
    "fastapi==0.109.0\n",
    "uvicorn[standard]==0.27.0\n",
    "\n",
    "# Database\n",
    "sqlalchemy==2.0.25\n",
    "\n",
    "# Data Validation  \n",
    "pydantic==2.5.3\n",
    "pydantic-settings==2.1.0\n",
    "\n",
    "# Environment\n",
    "python-dotenv==1.0.0\n",
    "\n",
    "# AI Integration (Gemini 2.5 Flash)\n",
    "google-generativeai==0.3.2\n",
    "\n",
    "# Database Drivers\n",
    "psycopg2-binary==2.9.9\n",
    "\n",
    "# Data Processing\n",
    "pandas==2.1.4\n",
    "numpy==1.26.3\n",
    "\n",
    "# API Utilities\n",
    "python-multipart==0.0.6\n",
    "\"\"\"\n",
    "        \n",
    "        with open(self.root / \"requirements.txt\", 'w', encoding='utf-8') as f:\n",
    "            f.write(requirements)\n",
    "        print(\"  âœ“ Created: requirements.txt\")\n",
    "        \n",
    "        # .env.example\n",
    "        env_example = \"\"\"# ImaraFund Environment Configuration\n",
    "\n",
    "# Database\n",
    "DATABASE_URL=sqlite:///./imarafund.db\n",
    "\n",
    "# API Configuration  \n",
    "PROJECT_NAME=ImaraFund\n",
    "API_V1_PREFIX=/api/v1\n",
    "DEBUG=True\n",
    "\n",
    "# AI Configuration\n",
    "GEMINI_API_KEY=your_gemini_api_key_here\n",
    "\n",
    "# Matching Algorithm Weights\n",
    "GEOGRAPHY_WEIGHT=0.40\n",
    "SECTOR_WEIGHT=0.30\n",
    "FUNDING_WEIGHT=0.20\n",
    "STAGE_WEIGHT=0.10\n",
    "\"\"\"\n",
    "        \n",
    "        with open(self.root / \".env.example\", 'w', encoding='utf-8') as f:\n",
    "            f.write(env_example)\n",
    "        print(\"  âœ“ Created: .env.example\")\n",
    "        \n",
    "        # README.md\n",
    "        readme = \"\"\"# ImaraFund\n",
    "\n",
    "AI-powered funding matcher for African SMEs.\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "Copy\n",
    "ImaraFund/ â”œâ”€â”€ app/ # FastAPI application â”‚ â”œâ”€â”€ api/ # API endpoints â”‚ â”œâ”€â”€ services/ # Business logic (matching, AI) â”‚ â””â”€â”€ core/ # Configuration â”œâ”€â”€ data/ # Data files â”‚ â”œâ”€â”€ grants/ # Grant datasets â”‚ â”œâ”€â”€ companies/ # Company datasets â”‚ â””â”€â”€ cleaned/ # Cleaned data outputs â”œâ”€â”€ migrations/ # Database migration scripts â”œâ”€â”€ tests/ # Unit tests â”œâ”€â”€ docs/ # Documentation â””â”€â”€ static/ # Static files\n",
    "\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Add backend code to `app/` folder\n",
    "2. Place CSV files in `data/grants/` and `data/companies/`\n",
    "3. Create migration scripts in `migrations/`\n",
    "4. Install dependencies: `pip install -r requirements.txt`\n",
    "\n",
    "## Archive\n",
    "\n",
    "Old files are moved to `archive/` with timestamps for safety.\n",
    "\"\"\"\n",
    "        \n",
    "        with open(self.root / \"README.md\", 'w', encoding='utf-8') as f:\n",
    "            f.write(readme)\n",
    "        print(\"  âœ“ Created: README.md\")\n",
    "        \n",
    "        print(\"âœ… Project files created!\\n\")\n",
    "    \n",
    "    def archive_old_files(self):\n",
    "        \"\"\"Archive old files while preserving Control_V2.ipynb and new structure.\"\"\"\n",
    "        print(\"ðŸ“¦ Archiving old files...\")\n",
    "        print(\"  â„¹ï¸  Preserving Control_V2.ipynb (you're working from it)\")\n",
    "        \n",
    "        archive_folder = self.root / \"archive\" / self.timestamp\n",
    "        archive_folder.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Files that MUST STAY in the root\n",
    "        preserve = {\n",
    "            \"Control_V2.ipynb\",       # Your working notebook\n",
    "            \"setup_imarafund.py\",     # This script\n",
    "            \"app\", \"data\", \"migrations\", \"archive\", \"tests\", \"docs\", \"static\",  # New structure\n",
    "            \".git\", \".gitignore\", \".env\",  # Git and config\n",
    "            \"requirements.txt\", \".env.example\", \"README.md\",  # Project files\n",
    "            \"venv\", \".venv\", \"ENV\", \"env\"  # Virtual environments\n",
    "        }\n",
    "        \n",
    "        archived_count = 0\n",
    "        important_files = []\n",
    "        \n",
    "        for item in self.root.iterdir():\n",
    "            if item.name in preserve:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                dest = archive_folder / item.name\n",
    "                \n",
    "                if item.is_file():\n",
    "                    shutil.move(str(item), str(dest))\n",
    "                elif item.is_dir():\n",
    "                    shutil.move(str(item), str(dest))\n",
    "                \n",
    "                # Track important files\n",
    "                if item.suffix in ['.ipynb', '.py', '.csv', '.json', '.xlsx']:\n",
    "                    important_files.append(item.name)\n",
    "                    print(f\"  âœ“ Archived (Important): {item.name}\")\n",
    "                else:\n",
    "                    print(f\"  âœ“ Archived: {item.name}\")\n",
    "                \n",
    "                archived_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  âš  Could not archive {item.name}: {e}\")\n",
    "        \n",
    "        if important_files:\n",
    "            print(f\"\\n  ðŸ“‹ Important files safely archived:\")\n",
    "            for file in important_files:\n",
    "                print(f\"     â€¢ {file}\")\n",
    "        \n",
    "        print(f\"\\nâœ… Archived {archived_count} items to archive/{self.timestamp}\")\n",
    "        print(\"âœ… Control_V2.ipynb preserved in root\\n\")\n",
    "    \n",
    "    def run_setup(self):\n",
    "        \"\"\"Execute the complete structure setup.\"\"\"\n",
    "        print(\"ðŸŽ¯ Starting structure-only setup...\\n\")\n",
    "        \n",
    "        # Step 1: Create folder structure\n",
    "        self.create_folder_structure()\n",
    "        \n",
    "        # Step 2: Create .gitignore\n",
    "        self.create_gitignore()\n",
    "        \n",
    "        # Step 3: Create project files\n",
    "        self.create_project_files()\n",
    "        \n",
    "        # Step 4: Archive confirmation\n",
    "        print(\"=\" * 70)\n",
    "        print(\"ARCHIVE CONFIRMATION\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"Archive old files while keeping Control_V2.ipynb in root?\")\n",
    "        print(\"(Your notebook will be safe - only other files will be moved)\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        response = input(\"\\nProceed with archiving? (y/n): \").lower().strip()\n",
    "        \n",
    "        if response == 'y':\n",
    "            self.archive_old_files()\n",
    "        else:\n",
    "            print(\"â­ï¸ Skipping archive step.\\n\")\n",
    "        \n",
    "        # Final summary\n",
    "        print(\"=\" * 70)\n",
    "        print(\"âœ… IMARAFUND STRUCTURE SETUP COMPLETE!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"\\nðŸ“‹ Created:\")\n",
    "        print(\"  âœ“ FastAPI folder structure\")\n",
    "        print(\"  âœ“ .gitignore (archive/ ignored)\")\n",
    "        print(\"  âœ“ requirements.txt\")\n",
    "        print(\"  âœ“ .env.example\")\n",
    "        print(\"  âœ“ README.md\")\n",
    "        \n",
    "        if response == 'y':\n",
    "            print(f\"  âœ“ Old files archived to: archive/{self.timestamp}/\")\n",
    "            print(\"  âœ“ Control_V2.ipynb kept in root\")\n",
    "        \n",
    "        print(\"\\nðŸ“‚ Your clean structure:\")\n",
    "        print(\"  ImaraFund/\")\n",
    "        print(\"  â”œâ”€â”€ Control_V2.ipynb     (your notebook)\")\n",
    "        print(\"  â”œâ”€â”€ app/                 (backend code goes here)\")\n",
    "        print(\"  â”œâ”€â”€ data/                (CSV files go here)\")\n",
    "        print(\"  â”œâ”€â”€ migrations/          (migration scripts)\")\n",
    "        print(\"  â”œâ”€â”€ archive/             (old files, git-ignored)\")\n",
    "        print(\"  â”œâ”€â”€ .gitignore\")\n",
    "        print(\"  â”œâ”€â”€ requirements.txt\")\n",
    "        print(\"  â””â”€â”€ README.md\")\n",
    "        \n",
    "        print(\"\\nðŸ“‹ Next Steps:\")\n",
    "        print(\"  1. Continue working from Control_V2.ipynb\")\n",
    "        print(\"  2. When ready, request the 10-step backend implementation\")\n",
    "        print(\"  3. Copy backend files into the app/ structure\")\n",
    "        print(\"  4. Place your CSV files in data/grants/ and data/companies/\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "\n",
    "\n",
    "# Run the setup\n",
    "if __name__ == \"__main__\":\n",
    "    setup = ImaraFundStructureSetup()\n",
    "    setup.run_setup()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12db0a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ§¹ ImaraFund Grants Data Cleaning\n",
      "======================================================================\n",
      "ðŸ“„ Input:  data/grants/final_merged_enhanced.csv\n",
      "ðŸ’¾ Output: data/cleaned/grants_cleaned.csv\n",
      "----------------------------------------------------------------------\n",
      "âœ“ Loaded with UTF-8 encoding\n",
      "âœ“ Loaded 103 grants with 63 columns\n",
      "\n",
      "ðŸ”— PRIMARY FIX: Filling data_source_url from website_url...\n",
      "  ðŸ“‰ Missing data_source_url BEFORE: 83 (80.6%)\n",
      "  ðŸ“‰ Missing website_url: 20 (19.4%)\n",
      "  âœ… Filled 83 data_source_url values from website_url\n",
      "  âœ… Missing data_source_url AFTER: 0 (0.0%)\n",
      "\n",
      "ðŸ’° Cleaning currency columns...\n",
      "  âœ“ estimated_value_amount: 102 â†’ 95 valid values\n",
      "  âœ“ minimum_amount: 20 â†’ 20 valid values\n",
      "  âœ“ maximum_amount: 20 â†’ 20 valid values\n",
      "  âœ“ minimum_revenue: 63 â†’ 61 valid values\n",
      "  âœ“ maximum_revenue: 63 â†’ 62 valid values\n",
      "\n",
      "âœ“ Standardizing boolean columns...\n",
      "  âœ“ Standardized 19 boolean columns to True/False\n",
      "\n",
      "ðŸ”¢ Cleaning integer columns...\n",
      "  âœ“ Validated 6 integer columns\n",
      "\n",
      "ðŸ“ Cleaning text columns...\n",
      "  âœ“ Cleaned 6 text columns\n",
      "\n",
      "ðŸ’¾ Saving cleaned dataset...\n",
      "  âœ“ Saved to: data/cleaned/grants_cleaned.csv\n",
      "  âœ“ Latest version: grants_cleaned_latest.csv\n",
      "\n",
      "ðŸ“Š Data Quality Summary:\n",
      "----------------------------------------------------------------------\n",
      "  ðŸ“ˆ Overall Completeness: 73.2%\n",
      "  ðŸ“Š Total Records: 103\n",
      "  ðŸ“‹ Total Columns: 63\n",
      "\n",
      "  ðŸŽ¯ Key Matching Columns Status:\n",
      "    âœ… data_source_url: 0 missing (0.0%) - Data source (FIXED)\n",
      "    âœ“ website_url: 20 missing (19.4%) - Website contact\n",
      "    âœ… estimated_value_amount: 8 missing (7.8%) - Funding amount\n",
      "    âœ… target_sectors: 0 missing (0.0%) - Sector matching\n",
      "    âœ… country: 0 missing (0.0%) - Geography matching\n",
      "    âœ… geographic_scope: 0 missing (0.0%) - Geography matching\n",
      "\n",
      "======================================================================\n",
      "âœ… DATA CLEANING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "ðŸ“‹ Summary of Changes:\n",
      "  âœ“ Fixed data_source_url (primary requirement)\n",
      "  âœ“ Cleaned currency columns (removed $, commas)\n",
      "  âœ“ Standardized 19 boolean columns\n",
      "  âœ“ Validated integer columns\n",
      "  âœ“ Cleaned text columns\n",
      "\n",
      "ðŸ“ Output Files:\n",
      "  â€¢ grants_cleaned.csv\n",
      "  â€¢ grants_cleaned_latest.csv (for easy access)\n",
      "\n",
      "ðŸ“‹ Next Steps:\n",
      "  1. Review the cleaned data in: data\\cleaned\n",
      "  2. Use grants_cleaned_latest.csv for backend migration\n",
      "  3. Request Part 2 (10-step backend implementation)\n",
      "\n",
      "======================================================================\n",
      "\n",
      "ðŸŽ‰ Cleaning completed! Output saved to: data\\cleaned\\grants_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ImaraFund Grants Data Cleaning Script\n",
    "====================================\n",
    "Primary fix: Fill data_source_url from website_url (83 missing â†’ ~20 missing)\n",
    "Plus comprehensive cleaning for all 63 columns based on CSV audit report.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "def clean_currency_value(value):\n",
    "    \"\"\"\n",
    "    Clean currency values: remove $, commas, spaces and convert to float.\n",
    "    Returns None for invalid/empty values (not 0.0 to preserve data integrity).\n",
    "    \"\"\"\n",
    "    if pd.isna(value) or value == \"\" or value is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Handle numeric types directly\n",
    "        if isinstance(value, (int, float)):\n",
    "            return float(value) if value != 0 else None\n",
    "        \n",
    "        # Clean string representations\n",
    "        cleaned = str(value).replace(\"$\", \"\").replace(\",\", \"\").replace(\" \", \"\").strip()\n",
    "        if not cleaned or cleaned.lower() in ['nan', 'null', 'none', '']:\n",
    "            return None\n",
    "        return float(cleaned)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def standardize_boolean(value):\n",
    "    \"\"\"\n",
    "    Convert various boolean representations to True/False.\n",
    "    Handles: Yes/No, True/False, 1/0, Y/N, On/Off\n",
    "    \"\"\"\n",
    "    if pd.isna(value) or value == \"\" or value is None:\n",
    "        return False\n",
    "    \n",
    "    if isinstance(value, bool):\n",
    "        return value\n",
    "    \n",
    "    str_val = str(value).lower().strip()\n",
    "    return str_val in [\"true\", \"yes\", \"1\", \"y\", \"on\", \"t\"]\n",
    "\n",
    "\n",
    "def clean_text_field(value, max_length=None):\n",
    "    \"\"\"Clean text fields: trim whitespace, handle nulls, enforce length limits.\"\"\"\n",
    "    if pd.isna(value) or value is None:\n",
    "        return None\n",
    "    \n",
    "    result = str(value).strip()\n",
    "    if not result or result.lower() in ['nan', 'null', 'none']:\n",
    "        return None\n",
    "    \n",
    "    if max_length and len(result) > max_length:\n",
    "        result = result[:max_length-3] + \"...\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def clean_grants_dataset(\n",
    "    input_path=\"data/grants/final_merged_enhanced.csv\",\n",
    "    output_path=\"data/cleaned/grants_cleaned.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Main cleaning function that processes the grants dataset.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ðŸ§¹ ImaraFund Grants Data Cleaning\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"ðŸ“„ Input:  {input_path}\")\n",
    "    print(f\"ðŸ’¾ Output: {output_path}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Ensure input file exists\n",
    "    input_file = Path(input_path)\n",
    "    if not input_file.exists():\n",
    "        raise FileNotFoundError(f\"Grants CSV not found: {input_path}\")\n",
    "    \n",
    "    # Load dataset with encoding fallback\n",
    "    try:\n",
    "        df = pd.read_csv(input_file, encoding='utf-8')\n",
    "        print(\"âœ“ Loaded with UTF-8 encoding\")\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            df = pd.read_csv(input_file, encoding='latin-1')\n",
    "            print(\"âœ“ Loaded with Latin-1 encoding\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Could not read CSV: {e}\")\n",
    "    \n",
    "    print(f\"âœ“ Loaded {len(df)} grants with {len(df.columns)} columns\\n\")\n",
    "    \n",
    "    # =================================================================\n",
    "    # PRIMARY FIX: Fill data_source_url from website_url\n",
    "    # =================================================================\n",
    "    print(\"ðŸ”— PRIMARY FIX: Filling data_source_url from website_url...\")\n",
    "    \n",
    "    # Check if columns exist\n",
    "    has_website = 'website_url' in df.columns\n",
    "    has_data_source = 'data_source_url' in df.columns\n",
    "    \n",
    "    if not has_data_source:\n",
    "        df['data_source_url'] = None\n",
    "        print(\"  â„¹ï¸  Created data_source_url column\")\n",
    "    \n",
    "    if has_website:\n",
    "        missing_before = df['data_source_url'].isna().sum()\n",
    "        missing_website = df['website_url'].isna().sum()\n",
    "        \n",
    "        print(f\"  ðŸ“‰ Missing data_source_url BEFORE: {missing_before} ({missing_before/len(df)*100:.1f}%)\")\n",
    "        print(f\"  ðŸ“‰ Missing website_url: {missing_website} ({missing_website/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        # Fill using combine_first (preserves existing data_source_url values)\n",
    "        df['data_source_url'] = df['data_source_url'].combine_first(df['website_url'])\n",
    "        \n",
    "        missing_after = df['data_source_url'].isna().sum()\n",
    "        filled = missing_before - missing_after\n",
    "        \n",
    "        print(f\"  âœ… Filled {filled} data_source_url values from website_url\")\n",
    "        print(f\"  âœ… Missing data_source_url AFTER: {missing_after} ({missing_after/len(df)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"  âš ï¸  website_url column not found\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # =================================================================\n",
    "    # SECONDARY FIXES: Clean dirty data types from audit report\n",
    "    # =================================================================\n",
    "    \n",
    "    # 1. Clean currency/numeric columns (currently objects with $ and commas)\n",
    "    print(\"ðŸ’° Cleaning currency columns...\")\n",
    "    currency_columns = [\n",
    "        'estimated_value_amount', 'minimum_amount', 'maximum_amount',\n",
    "        'minimum_revenue', 'maximum_revenue'\n",
    "    ]\n",
    "    \n",
    "    for col in currency_columns:\n",
    "        if col in df.columns:\n",
    "            non_null_before = df[col].notna().sum()\n",
    "            df[col] = df[col].apply(clean_currency_value)\n",
    "            non_null_after = df[col].notna().sum()\n",
    "            print(f\"  âœ“ {col}: {non_null_before} â†’ {non_null_after} valid values\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # 2. Standardize boolean columns (mixed string representations)\n",
    "    print(\"âœ“ Standardizing boolean columns...\")\n",
    "    boolean_columns = [\n",
    "        'repayment_required', 'environmental_focus', 'innovation_focus',\n",
    "        'digital_focus', 'export_focus', 'women_focused', 'youth_focused',\n",
    "        'agriculture_focused', 'green_climate_focused', 'technical_assistance',\n",
    "        'mentorship_available', 'networking_opportunities', 'training_provided',\n",
    "        'co_financing_required', 'co_financing_available', 'export_support',\n",
    "        'technology_innovation', 'digital_application', 'verified'\n",
    "    ]\n",
    "    \n",
    "    standardized_count = 0\n",
    "    for col in boolean_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(standardize_boolean)\n",
    "            standardized_count += 1\n",
    "    \n",
    "    print(f\"  âœ“ Standardized {standardized_count} boolean columns to True/False\")\n",
    "    print()\n",
    "    \n",
    "    # 3. Clean integer columns (handle mixed types)\n",
    "    print(\"ðŸ”¢ Cleaning integer columns...\")\n",
    "    integer_columns = [\n",
    "        'duration_months', 'minimum_employees', 'maximum_employees',\n",
    "        'grace_period_months', 'total_beneficiaries', 'year_established'\n",
    "    ]\n",
    "    \n",
    "    for col in integer_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')\n",
    "    \n",
    "    print(f\"  âœ“ Validated {len([c for c in integer_columns if c in df.columns])} integer columns\")\n",
    "    print()\n",
    "    \n",
    "    # 4. Clean critical text columns\n",
    "    print(\"ðŸ“ Cleaning text columns...\")\n",
    "    text_cleaning = {\n",
    "        'program_name': 500,\n",
    "        'institution_name': 500,\n",
    "        'eligibility_criteria': None,\n",
    "        'application_process': None,\n",
    "        'notes': None,\n",
    "        'special_features': None\n",
    "    }\n",
    "    \n",
    "    for col, max_len in text_cleaning.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(lambda x: clean_text_field(x, max_len))\n",
    "    \n",
    "    print(f\"  âœ“ Cleaned {len([c for c in text_cleaning.keys() if c in df.columns])} text columns\")\n",
    "    print()\n",
    "    \n",
    "    # =================================================================\n",
    "    # SAVE CLEANED DATA\n",
    "    # =================================================================\n",
    "    print(\"ðŸ’¾ Saving cleaned dataset...\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_file = Path(output_path)\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save cleaned data\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(f\"  âœ“ Saved to: {output_path}\")\n",
    "    \n",
    "    # Also save as 'latest' for easy reference\n",
    "    latest_path = output_file.parent / \"grants_cleaned_latest.csv\"\n",
    "    df.to_csv(latest_path, index=False, encoding='utf-8')\n",
    "    print(f\"  âœ“ Latest version: {latest_path.name}\")\n",
    "    \n",
    "    # =================================================================\n",
    "    # GENERATE SUMMARY REPORT\n",
    "    # =================================================================\n",
    "    print(\"\\nðŸ“Š Data Quality Summary:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Missing data analysis\n",
    "    missing_data = df.isna().sum()\n",
    "    total_cells = len(df) * len(df.columns)\n",
    "    filled_cells = df.notna().sum().sum()\n",
    "    completeness = (filled_cells / total_cells) * 100\n",
    "    \n",
    "    print(f\"  ðŸ“ˆ Overall Completeness: {completeness:.1f}%\")\n",
    "    print(f\"  ðŸ“Š Total Records: {len(df):,}\")\n",
    "    print(f\"  ðŸ“‹ Total Columns: {len(df.columns)}\")\n",
    "    \n",
    "    # Key columns status\n",
    "    key_columns = {\n",
    "        'data_source_url': 'Data source (FIXED)',\n",
    "        'website_url': 'Website contact',\n",
    "        'estimated_value_amount': 'Funding amount',\n",
    "        'target_sectors': 'Sector matching',\n",
    "        'country': 'Geography matching',\n",
    "        'geographic_scope': 'Geography matching'\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  ðŸŽ¯ Key Matching Columns Status:\")\n",
    "    for col, desc in key_columns.items():\n",
    "        if col in df.columns:\n",
    "            missing = missing_data[col]\n",
    "            missing_pct = (missing / len(df)) * 100\n",
    "            status = \"âœ…\" if missing_pct < 10 else \"âœ“\" if missing_pct < 30 else \"âš ï¸\"\n",
    "            print(f\"    {status} {col}: {missing} missing ({missing_pct:.1f}%) - {desc}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"âœ… DATA CLEANING COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nðŸ“‹ Summary of Changes:\")\n",
    "    print(f\"  âœ“ Fixed data_source_url (primary requirement)\")\n",
    "    print(f\"  âœ“ Cleaned currency columns (removed $, commas)\")\n",
    "    print(f\"  âœ“ Standardized {standardized_count} boolean columns\")\n",
    "    print(f\"  âœ“ Validated integer columns\")\n",
    "    print(f\"  âœ“ Cleaned text columns\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ Output Files:\")\n",
    "    print(f\"  â€¢ {output_file.name}\")\n",
    "    print(f\"  â€¢ grants_cleaned_latest.csv (for easy access)\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Next Steps:\")\n",
    "    print(f\"  1. Review the cleaned data in: {output_file.parent}\")\n",
    "    print(f\"  2. Use grants_cleaned_latest.csv for backend migration\")\n",
    "    print(f\"  3. Request Part 2 (10-step backend implementation)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    return df, output_file\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# USAGE FUNCTIONS\n",
    "# =================================================================\n",
    "\n",
    "def run_from_notebook():\n",
    "    \"\"\"Convenience function for running from Control_V2.ipynb\"\"\"\n",
    "    return clean_grants_dataset()\n",
    "\n",
    "\n",
    "def run_with_custom_paths(input_csv, output_csv):\n",
    "    \"\"\"Run with custom file paths\"\"\"\n",
    "    return clean_grants_dataset(input_csv, output_csv)\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# MAIN EXECUTION\n",
    "# =================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run with default paths\n",
    "    cleaned_df, output_path = clean_grants_dataset()\n",
    "    print(f\"ðŸŽ‰ Cleaning completed! Output saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d541f43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
